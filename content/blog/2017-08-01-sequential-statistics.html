---
title: Sequential Statistics
author: Erik-Jan van Kesteren
date: '2017-08-01'
slug: sequential-statistics
categories:
  - R
  - Statistics
tags:
  - R
  - Statistics
  - Computation
---



<div id="the-problem" class="section level2">
<h2>The problem</h2>
<p>How do we calculate a sample mean? This is probably one of the most basic questions in statistics, with as its common answer the following: Given a vector of sample values <span class="math inline">\(\bf{x}\)</span> of length <span class="math inline">\(N\)</span>, the sample mean <span class="math inline">\(m\)</span> is defined as <span class="math display">\[m = \frac{1}{N}\sum^N_{i=1}x_i\]</span></p>
<p>But this first assumption, <em>given a vector of sample values</em>, recently did not hold for me. The sheer size of the vector that I needed to process made sure it did not fit on my computer. Even worse, I had no way of knowing how big this vector would exactly be! My question was thus: how do I calculate the mean of a stream of sample values of indetermined length?</p>
</div>
<div id="solution" class="section level2">
<h2>Solution</h2>
<p>The solution to this was rather simple for calculation of the mean. We initialise the value of the mean to 0, and then <em>update</em> our current guess of the mean with the next value’s weighted deviation from the current mean. This happens like so:</p>
<p><span class="math display">\[m_{i} = m_{i-1} + \frac{(x_i-m_{i-1})}{i}\]</span></p>
<p>Reading the formula in words, the <span class="math inline">\(i^{th}\)</span> mean is the <span class="math inline">\((i-1)^{th}\)</span> mean plus the deviation of the <span class="math inline">\(i^{th}\)</span> input value from this current mean divided by the current <span class="math inline">\(i\)</span>, i.e., the amount of values that have been input.</p>
<p>In code this is simple to implement:</p>
<pre class="r"><code>set.seed(142857)
# let&#39;s assume we receive data of length 1234
streamlength &lt;- 1234

# initialise m_i
m_i &lt;- 0
for (i in 1:streamlength) {
    m_prev &lt;- m_i
    x_i &lt;- rnorm(1, 0, 3)
    m_i &lt;- m_prev + (x_i - m_prev)/i
}

print(m_i)</code></pre>
<pre><code>## [1] -0.00210514</code></pre>
<p>Note that in the above code we never save the full vector <span class="math inline">\(\bf{x}\)</span>; we only ever save the current and previous versions of the mean. This is perfect for an extremely large, variable length input vector such as the one I talked about in the introduction!</p>
<pre class="r"><code>set.seed(142857)
# let&#39;s assume we receive data of length 1234
streamlength &lt;- 1234

# initialise m_i
means &lt;- numeric(streamlength)
for (i in 1:streamlength) {
  x_i &lt;- rnorm(1, 0, 3)
  means[i] &lt;- ifelse(i == 1,
                     x_i,
                     means[i-1] + (x_i-means[i-1]) / i)
}</code></pre>
<p><img src="/blog/2017-08-01-sequential-statistics_files/figure-html/seqplot-1.svg" width="672" /></p>
<p>Notice the big changes at the start of the stream, and the smaller changes at the end, asymptotically converging to the “true” mean value that we set here at 0.</p>
</div>
<div id="bayesian-estimation" class="section level2">
<h2>Bayesian estimation?</h2>
<p>We can also see this as a form of bayesian updating, if we turn the formula around like so:</p>
<p><span class="math display">\[m_{i} = \frac{(x_i-m_{i-1}) + i \cdot m_{i-1}}{i}\]</span></p>
<p>Here, we set the prior to be <span class="math inline">\(i \cdot m_{i-1}\)</span>, then we see <span class="math inline">\((x_i-m_{i-1})\)</span> as our new data/evidence, <span class="math inline">\(m_i\)</span> is our posterior, and <span class="math inline">\(i\)</span> is the normalising constant. Cool!</p>
</div>
<div id="update-more-efficient" class="section level2">
<h2>Update: More efficient!</h2>
<p>The algorithm above translates nicely into the bayesian framework, but as with so many algorithms, it can be made much more efficient. It turns out that all we have to do is remember the <code>sum</code> of the values input in the stream and a counter <code>i</code> that indicates how many values went in. Then, when asking for the mean, all we need to do is <span class="math inline">\(m_i=\frac{\texttt{sum}}{\texttt{i}}\)</span>. Simple!</p>
<p>This is better for three reasons:</p>
<ol style="list-style-type: decimal">
<li>It’s simpler.</li>
<li>It’s less prone to numerical problems with your computer: you only perform one operation.</li>
<li>This also extends to the variance and higher-order moments. For variance, we need to remember the <code>sum</code>, the <code>sum of squares</code>, and the counter <code>i</code>. Then, we can calculate the variance using the formula <span class="math inline">\(Var(X) = E[X^2] - (E[X])^2\)</span> to calculate the variance: <span class="math inline">\(s^2_i=\frac{\texttt{sum of squares}}{\texttt{i}}-\left(\frac{\texttt{sum}}{\texttt{i}}\right)^2\)</span>. For each higher order moment, we need to remember a higher power sum in this framework.</li>
</ol>
<p>Let’s do it!</p>
<pre class="r"><code>set.seed(142857)
# let&#39;s assume we receive data of length 1234
streamlength &lt;- 1234

# initialise
sum &lt;- 0
sumsq &lt;- 0
i &lt;- 0

for (j in 1:streamlength) {
  value &lt;- rnorm(1, 0, 3)
  sum &lt;- sum + value
  sumsq &lt;- sumsq + value^2
  i &lt;- i + 1
}

list(sum = sum, 
     sum_of_squares = sumsq, 
     i = i, 
     mean = sum/i,
     variance = sumsq/i - (sum/i)^2)</code></pre>
<pre><code>## $sum
## [1] -2.597743
## 
## $sum_of_squares
## [1] 10948.2
## 
## $i
## [1] 1234
## 
## $mean
## [1] -0.00210514
## 
## $variance
## [1] 8.872118</code></pre>
</div>
<div id="the-question" class="section level2">
<h2>The question</h2>
<p>For the mean this was simple to implement. The question I’m pondering in the back of my mind throughout all this is the following: can <em>any</em> statistic be transformed into such a sequential statistic? How does this work for variance? Standard deviation? The median / other quantiles? If you let me know, I’ll be sure to update this blog post with the additions.</p>
</div>
